[{"id":"oraculopipeline","user_id":"bf566535-914d-4cf1-8b9d-a32a572c2410","name":"Oraculo","type":"pipe","content":"from typing import Optional, Callable, Awaitable\nimport time\nimport requests\nfrom pydantic import BaseModel, Field\n\n\nclass Pipe:\n\n    class Valves(BaseModel):\n        \"\"\"Configurações editáveis no painel do OpenWebUI.\"\"\"\n\n        api_url: str = Field(\n            default=\"http://back-end:8000/ask\",\n            description=\"Endpoint da API FastAPI que recebe a pergunta (POST).\",\n        )\n        bearer_token: str = Field(\n            default=\"\",\n            description=\"Token Bearer opcional para autenticação na API.\",\n        )\n        emit_interval: float = Field(\n            default=2.0,\n            description=\"Intervalo, em segundos, entre atualizações de status no UI.\",\n        )\n        enable_status_indicator: bool = Field(\n            default=True,\n            description=\"Ativa ou desativa a barra de progresso no chat.\",\n        )\n        max_file_size: int = Field(\n            default=1048576,\n            description=\"Tamanho máximo (bytes) para arquivos recebidos — 1 MB por padrão.\",\n        )\n\n    def __init__(self):\n        self.type = \"pipe\"\n        self.id = \"fastapi_pipe\"\n        self.name = \"FastAPI Pipe\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0.0  # controle de throttle dos status\n\n    # --------------------------------------------------------------\n    # Utilitário para reportar progresso para o OpenWebUI\n    # --------------------------------------------------------------\n    async def _emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool = False,\n    ) -> None:\n        now = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (now - self.last_emit_time >= self.valves.emit_interval or done)\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = now\n\n    # --------------------------------------------------------------\n    # Método principal chamado pelo OpenWebUI\n    # --------------------------------------------------------------\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,  # não usado\n    ) -> Optional[dict]:\n        \"\"\"Envia a última mensagem do usuário à API FastAPI e retorna a resposta.\"\"\"\n\n        # Recupera a última mensagem do usuário\n        await self._emit_status(\n            __event_emitter__, \"info\", \"Processando entrada...\", False\n        )\n\n        messages = body.get(\"messages\", [])\n        if not messages:\n            await self._emit_status(\n                __event_emitter__, \"error\", \"Nenhuma mensagem encontrada\", True\n            )\n            return {\"error\": \"Nenhuma mensagem encontrada\"}\n\n        last_content = messages[-1][\"content\"]\n        # Estrutura de content pode ser: str  | list[ {type: text|file} ]\n        question = self._extract_text(last_content)\n\n        # Monta request para API FastAPI\n        headers = {\"Content-Type\": \"application/json\"}\n        if self.valves.bearer_token:\n            headers[\"Authorization\"] = f\"Bearer {self.valves.bearer_token}\"\n\n        payload = {\"question\": question}\n\n        await self._emit_status(\n            __event_emitter__, \"info\", \"Chamando API FastAPI...\", False\n        )\n\n        try:\n            response = requests.post(\n                self.valves.api_url,\n                json=payload,\n                headers=headers,\n                timeout=120,\n            )\n            response.raise_for_status()\n        except requests.RequestException as exc:\n            await self._emit_status(__event_emitter__, \"error\", f\"Erro: {exc}\", True)\n            return {\"error\": str(exc)}\n\n        # Interpreta resposta da API.\n        try:\n            data = response.json()\n        except ValueError:\n            data = {\"output\": response.text}\n\n        answer = data.get(\"output\") or data\n\n        # Acrescenta a resposta ao histórico e devolve ao WebUI\n        body[\"messages\"].append({\"role\": \"assistant\", \"content\": answer})\n\n        await self._emit_status(__event_emitter__, \"info\", \"Resposta entregue\", True)\n        return answer\n\n    # --------------------------------------------------------------\n    # Auxiliar: extrair texto da mensagem (lida com arquivos/objetos)\n    # --------------------------------------------------------------\n    def _extract_text(self, content) -> str:\n        if isinstance(content, str):\n            return content.replace(\"Prompt: \", \"\", 1).strip()\n\n        # Caso seja uma lista (arquivos + texto)\n        text_found = \"\"\n        file_detected = False\n        for item in content:\n            if isinstance(item, dict):\n                if item.get(\"type\") == \"text\" and item.get(\"text\"):\n                    text_found = item[\"text\"].strip()\n                elif item.get(\"type\") == \"file\" and not text_found:\n                    file_detected = True\n                    size = item.get(\"size\", 0)\n                    name = item.get(\"name\", \"arquivo\")\n                    if size > self.valves.max_file_size:\n                        text_found = f\"Recebemos o arquivo {name}, mas ele é muito grande para ser processado.\"\n                    else:\n                        text_found = f\"Recebemos o arquivo {name}. Ainda não processamos arquivos neste chat.\"\n        return text_found or \"Arquivo recebido.\"\n","meta":{"description":"Envia as informações diretamente para o back-end","manifest":{}},"is_active":true,"is_global":false,"updated_at":1750446467,"created_at":1750445904}]